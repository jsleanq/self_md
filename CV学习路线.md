# CV学习路线

## 一、统计学习方法

- 内容：建立模型（感知机、回归模型、判别模型）基本概念
- 学习程度：
  1. 了解统计学习三要素、模型评估、模型选择；
  2. 了解判别模型和生成模型；
  3. 了解感知机、回归模型、分类模型的区别和原理；
  4. 熟悉泛化能力、分类问题、标注问题。
- 学习笔记：[参考链接](https://cloud.tencent.com/developer/article/1092037)
  1. 统计学习三要素、模型评估、模型选择
    - 统计学习=模型+策略+算法
        - 模型：考虑学习什么样的模型，**监督学习**中，模型就是所要学习的条件概率分布或决策函数，由决策函数表示的模型为非概率模型，由条件概率分布表示的模型为概率模型。
        - 策略：有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。监督学习实际上就是一个经验风险或者结构风险函数的最优化问题。风险函数度量平均意义下模型预测的好坏，模型每一次预测的好坏用损失函数来度量。
        - 算法：统计学习问题归结为以上的最优化问题，这样，统计学习的算法就是求解最优化问题的算法。如果最优化问题有显示的解析解，这个最优化问题就比较简单，但通常这个解析解不存在，所以就需要利用数值计算的方法来求解。统计学习可以利用已有的最优化算法，也可以开发独自的最优化算法。
    - 模型评估与模型选择
        - 评估标准：误差
          - 训练误差：模型关于训练数据集的平均损失(经验风险)
          - 测试误差：模型关于测试数据集的平均损失(经验风险)
          - 过拟合：训练效果好，测试效果差(泛化能力差,模型结构复杂)
        - 模型选择
          - 正则化：在经验风险上加一个正则化项或罚项，选择经验风险与模型复杂度同时较小的模型
          - 交叉验证：重复的使用数据，把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择
    - 泛化能力
        - 学习方法的泛化能力(generalization ability)是指由该方法学习到的模型对未知数据的预测能力。实际情况通常通过测试误差来评价学习方法的泛化能力。如果在不考虑数据量不足的情况下出现模型的泛化能力差的情况，那么一般都是由于对损失函数的优化没有达到全局最优。

  2. 判别模型和生成模型
  - 生成模型：学习到联合概率分布P(X,Y)，即特征x和标记y共同出现的概率，然后求条件概率分布P(Y|X)，之后P(Y|X)最大的类别就是最终预测的类别。
  - 判别模型：直接学习到决策函数f(x)，即对输入空间到输出空间的映射进行建模。
  - 判别模型输出的是根据X"判断"得到的Y，生成模型输出的是在X"生成"的所有Y中生成概率最大的Y。[参考链接](https://blog.csdn.net/Mr_health/article/details/107596402?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-107596402-blog-119297096.pc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-107596402-blog-119297096.pc_relevant_recovery_v2&utm_relevant_index=1)

  3. 感知机、分类模型、回归模型
  - 感知机是二分类的线性模型，其输入是实例的特征向量，输出的是实例的类别，分别是+1和-1，属于判别模型。
    - 如果训练数据集是线性可分的，则感知机一定能求得分离超平面。
    - 感知机学习的目标是求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面；损失函数是误分类点到超平面S的总距离；优化策略是随机梯度下降法。
    - 感知机的分类函数采用阶跃函数作为区分
  - 分类模型
    - 概念：对于分类问题，监督学习从数据中学习一个分类模型或者分类决策函数，称为分类器。分类器对新的输入预测其属于哪一类别，称为分类。
    - 优化过程：找到最优决策面
    - 输出：离散值，如0/1，yes/no
    - 评价指标：一般是精确率，即给定测试数据集，分类器能正确分类的样本数占总样本数的比。
    - 模型损失函数：交叉熵损失函数
  - 回归模型
    - 概念：回归用于预测输入变量和输出变量之间的关系，特别是当输入变量的值发生变化时，输出变量的值也会跟着变化。回归模型正是表示输入变量到输出变量之间的映射函数，回归问题的学习等价于函数拟合。
    - 优化过程：使得学习到的函数曲线跟真实曲线拟合的最好
    - 输出：连续值，如房子的售价，天气温度
    - 评价指标：根据回归任务的不同而不同，常用的包括RMSE(平方根误差)、MAE(平均绝对误差)、MSE(平均平方误差)、R2_score等。
    - 模型损失函数：常用的是均方误差

  4. 分类问题、标注问题、回归问题
  - **输入变量和输出变量均为连续变量**的预测问题称为**回归问题**；
      - 回归问题的学习等价于函数拟合：选择一条函数曲线使其很好的拟合已知数据且很好地预测未知数据。回归问题按照输入变量的个数分为一元回归和多元回归，按照输入变量和输出变量之间的关系分为线性回归和非线性回归。
  - **输出变量为有限个离散变量**的预测问题称为**分类问题**；
      - 分类问题主要分为二分类和多分类，对于二分类问题，常用的评价指标是精确率和召回率。
  - **输入变量与输出变量均为变量序列**的预测问题称为**标注问题**
      - 标注问题的输入是一个观测序列，输出是一个标记序列。标注问题在信息抽取、自然语言处理等领域被广泛采用。例如对一个单词序列预测其对应的词性标记序列。

## 二、深度学习基础

- 内容：卷积神经网络组成、原理
- 学习程度：
  1. 了解卷积作用、过程、优缺点，几种常见的卷积方式；
  2. 了解归一化的方法、作用、原理；
  3. 最大池化和平均池化的作用、区别、原理、前传后传是如何完成的、各自的应用场合；
  4. 常见的激活函数、特点、各自适用的场合；
  5. 了解正则化常用方法、作用、原理、特点、优缺点；
  6. 了解常见的损失函数、特点、各自适用的场合；
  7. 了解反向传播原理，手动推导卷积神经网络；
  8. 了解梯度下降原理，常见的优化方式。
- 学习笔记：[参考链接](https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/119604015)
  1. 神经网络组成
    - 神经网络，也指人工神经网络(Artificial Neural Networks，简称ANNs)，每个神经网络单元抽象出来的数学模型叫感知器，接收多个输入(x1，x2，x3...)，产生一个输出。
    - 实际的决策模型是由多个感知器组成的多层网络，这也是经典的神经网络模型，由输入层、隐含层、输出层构成。
    <center><img src="/assets/05d833e3642d8ed468f044a065dcb443.png" width=60%></center>

  2. 卷积神经网络(CNN)[参考链接](https://my.oschina.net/u/876354/blog/1620906)
    - CNN原理
      - 经典的神经网络模型需要读取整幅图像作为输入(全连接)，而卷积神经网络原理与人类对外界的认知类似，从局部到全局，依靠的则是局部感受野降低参数数目。
      - 在两幅图像中大致相同的位置找到一些粗糙的特征（小块图像）进行匹配，相比起传统的整幅图逐一比对的方式，CNN的这种小块匹配方式能够更好的比较两幅图像之间的相似性
    - 过程
      - 当给定一张新图时，CNN并不能准确地知道这些特征到底要匹配原图的哪些部分，所以它会在原图中把每一个可能的位置都尝试匹配，相当于把这个feature(特征)变成了一个过滤器。这个用来匹配的过程就被称为卷积操作。
      - 计算一个feature和其在原图上对应的某一小块的结果，只需将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可
      - 对多个特征图像不断地重复着上述过程，通过每一个feature的卷积操作，最终得到一个新的二维数组，称之为feature map。其中的值，越接近1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。
    - CNN优缺点
      - 优点：
        1. 参数共享。如果某个过滤器适用于图片的某个区域，它也可能适用于图片的其他区域
        2. 无需手动选取特征，训练好权重，即得特征分类效果好
      - 缺点：
        1. 需要调参，需要大样本量，训练最好要GPU
        2. 解释性差，既不了解提取的是什么特征，神经网络本身也是黑盒模型
    - 常见卷积方式[参考链接](https://blog.csdn.net/weixin_43112053/article/details/127374541)
      |卷积方式|优点|缺点|
      |:-:|:-:|:-:|
      |深度可分离卷积|可大幅度减少卷积的参数|使用不当模型能力下降 <br> 在GPU上内存访问量很高|
      |可变形卷积|可对不规则和旋转目标等检测|增加了计算量<br>可使用分组卷积降低计算量|
      |分组卷积|参数量是正常卷积的1/N <br> 可视为一种正则，防止过拟合|信息只存在本组，通道之间的信息没有交互|
      |空洞卷积|可以同时控制感受野和分辨率 <br> 获取多尺度上下文信息|损失信息的连续性 <br> 目标分割丢失小目标|
  3. 归一化
    - 归一化就是把所有数据都转化成[0,1]或者[-1,1]之间的数，其目的是为了取消各维数据之间的数量级差别，避免因为输入输出数据数量级差别大而造成网络预测误差过大。
    - 作用
      - 统一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准，这算是应用层面的需求。
      - 为了后面数据处的方便，归一化可以避免一些不必要的数值问题。
      - 为了程序运行时收敛速度更快。
      - 避免神经元饱和。当神经元的激活在接近0或者1时，在这些区域，梯度几乎为0，这样在反向传播过程中，局部梯度就会接近于0，非常不利于网络的训练。
    - 方法：
      1. 线性归一化 $$ x' = \frac{x-min(x)}{max(x)-min(x)} $$
      2. 标准差标准化 $$ x' = \frac{x-\mu}{\sigma} $$
      3. 非线性归一化 通过一些数学函数，将原始值进行映射。该方法包括log、指数，正切等
    - 批归一化(Batch Normalization)
      - 以往的神经网络训练只对输入层数据进行归一化处理，没有在中间层进行归一化处理，BN主要针对的就是中间层参数的归一化处理
      - 优势：
        1. 减少了人为选择参数
        2. 减少了对学习率的要求，我们可以使用初始状态下很大的学习率或者当使用较小的学习率时，算法也能够快速训练收敛。
        3. 破坏了原来的数据分布，一定程度上缓解了过拟合（防止每批训练中某一个样本经常被挑选到）
        4. 减少梯度消失，加快收敛速度，提高训练精度。
      - 使用场景：
        1. 在CNN中BN放在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。
        2. 一般使用BN来加快训练速度，提高模型精度。
        3. 当每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。

    - 群组归一化(Group Normalization)
      - GN将通道分成组，并在每组内计算归一化的均值和方差。GN的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。
  4. 最大池化和平均池化
    - 池化就是将输入图像进行缩小，减少像素信息，只保留重要信息，通过加入池化层，图像缩小了，能很大程度上减少计算量。也称为子采样或下采样。
    - 操作：
      - 按照池化区域，将区域内的值按一定规则转换成相应的值，例如取这个池化区域内的最大值（max-pooling）、平均值（mean-pooling）等，以这个值作为结果的像素值。
      - 最大池化
        - 最大池化可以提取特征纹理
        - 保留了每一小块内的最大值，也就是相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）
        - 不仅可以便宜地代替卷积，而且可以保证CNN的平移不变性，即不管特征所处的位置，只要匹配上就能提取。
        - 前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。
        - 反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0
      - 平均池化
        - 平均池化可以保留背景信息
        - 保留了有关块或池中“次重要”元素的大量信息
        - 前向传播就是把一个patch中的值求取平均来做pooling
        - 反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变。
      - 全局池化（global pooling）
        - 获取全局上下文关系。
        - 最大池化、平均池化均在feature map上以窗口的形式进行滑动，经过操作后feature map降采样，减少了过拟合现象。
        - 全局池化不以窗口的形式取均值，而是以feature map为单位进行均值化。即一个feature map输出一个值。
  5. 激活函数
   - 引入激活函数是为了增加神经网络模型的非线性，对模型学习、理解非常复杂和非线性的函数具有重要作用。
   - 作用
      - 激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。
      - 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。
    - sigmoid 激活函数
  6. 正则化[参考链接](https://blog.csdn.net/weixin_42990464/article/details/128169386)
  - L1、L2正则化[参考链接](https://blog.csdn.net/weixin_41960890/article/details/104891561)
    - 方法、作用、原理、特点、优缺点；
  7. 损失函数[参考链接](https://www.cvmart.net/community/detail/4879)
    - 、特点、各自适用的场合；
  8. 反向传播[参考链接](https://blog.csdn.net/johnny_love_1968/article/details/117598649?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-117598649-blog-119795266.pc_relevant_recovery_v2&spm=1001.2101.3001.4242.2&utm_relevant_index=4)
    - 原理，手动推导卷积神经网络；
  9. 梯度下降[参考链接](https://blog.csdn.net/JaysonWong/article/details/119818497)
    - 原理，常见的优化方式。

## 三、PyTorch搭建神经网络

- 内容：PyTorch使用，了解一个完整的模型由什么组成，建立计算机视觉的知识体系
- 学习程度：
  1. 了解一个完整模型的组成部分及作用；
  2. 学习定义classdataset，了解数据读取流程，学会预处理；
  3. 学会搭建基本的卷积神经网络；
  4. 学习编写训练过程，加载预训练模型，设置学习率优化方式；
  5. 学会推理，训练实时可视化，解析参数。
- 学习方式：公众号《从零搭建PyTorch模型》系列，之后搭建一个简单的分类网络。
- 学习目的：搭建一个完整的模型，了解一个模型的完整实现过程。

## 四、数字图像处理与OpenCV

- 内容：了解图像的一些基本知识，了解计算机视觉的传统解决方法，学习图像处理
- 学习程度：
  1. 基本的图像读取、保存、裁剪、转灰度、视频读取、保存；
  2. 图像基本处理方法、算数运算、图像阈值、平滑、滤波、形态学处理、梯度、边缘检测、轮廓检测；
  3. 直方图、图像增强、hough变换、分水岭算法、角点检测等；
  4. 传统特征提取方法（传统特征描述子），如HOG、SIFT、SURF、BRIEF、ORB等；
  5. 视频分析光流、meanshift、camshift等；
  6. 以上在opencv中的实现。
- 学习方式：《opencv-python-tutorial中文教程》。
- 学习目的：了解图像处理和传统解决方法，了解图像预处理方法。

## 五、卷积神经网络进阶

- 内容：学习现有经典卷积神经网络、轻量化网络、了解设计思路、设计原则
- 学习程度：
  1. 熟悉现有所有的经典卷积神经网络，如ALexNet、NiN、VGG、GoogleNet、Inception系列、ResNet系列，DenseNet系列，了解设计思路和设计原则；
  2. 熟悉现有所有的轻量化网络，如Xception、MobileNet系列、ShuffleNet系列、SqueezeNet、GhostNet、EfficientNet等；
- 学习方式：原论文英文版，阅读源码了解实现过程，争取完成综述。
- 学习目的：懂得经典神经网络的设计原则和模块设计方法，之后在自己的论文中的设计才有合理的解释。

## 六、目标检测

- 内容：了解计算机视觉的具体应用，根据应用创新改进模型
- 学习程度：
  1. 了解目标检测的传统检测算法；
  2. 熟悉RCNN系列发展演变，设计思路，改进思路，优缺点；
  3. 熟悉YOLO系列发展演变，设计思路，改进思路，优缺点；
  4. 熟悉anchor-free系列发展演变，设计思路，优缺点；
  5. 熟悉IoU系列的改进思路，NMS的基本实现和改进思路；
  6. 熟悉目标检测中的常用技术，如特征金字塔、注意力机制等；
  7. 熟悉目标检测中的评估指标，如FPS、mAP、召回率、ROC曲线、precision和accuracy等，熟悉指标计算方式；
  8. 能够复现其中任意一个模型的能力
- 学习方式：原论文英文版，阅读源码了解实现过程，争取完成综述，不懂的可以看看博客。
- 学习目的：彻底了解计算机视觉的研究过程，目标检测是计算机视觉的基本使用场景。

## 七、机器学习模型

- 内容：回顾二中的内容，学习剩余模型
- 学习程度：
  1. 熟悉第二步中的内容；
  2. 学习剩余模型，SVM、聚类、其他的了解基本原理、使用场景、优缺点；
  3. 了解梯度下降法、牛顿法和拟牛顿法、拉格朗日对偶性等知识；
- 学习方式：《统计学习方法》，公式了解，重点了解基本原理、使用场景、优缺点。
- 学习目的：扩充知识储备。

## 八、深度学习基础

- 内容：学习GAN、编码器、循环神经网络、自监督、无监督、少样本学习、元学习、迁移学习、知识蒸馏、图神经网络、图卷积神经网络等
- 学习程度：
  1. 熟悉GAN的基本原理、基本模型、训练过程、应用场合等；
  2. 熟悉编码器的基本原理、基本模型、应用场合等；
  3. 熟悉RNN、LSTM的基本原理、优缺点等；
  4. 了解自监督、无监督的基本原理、常用模型等；
  5. 熟悉少样本学习、元学习基本原理、常用方法等；
  6. 熟悉迁移学习、知识蒸馏基本原理、常用方法等；
  7. 了解图神经网络、图卷积神经网络基本原理、常用方法等；
- 学习方式：阅读相关综述文章阅读；
- 学习目的：扩充知识储备。

## 九、计算机视觉进阶

- 内容：总结深度学习、神经网络技术，了解热门研究现状、基本模型，如语义分割、实例分割、目标跟踪、行人重识别、行为识别、3DCNN模型、视频中的一些检测、Transformer模型
- 学习程度：
  1. 总结数据增强方法、注意力机制、特征金字塔、归一化方法、损失函数等；
  2. 掌握Tensorflow框架、具备看懂Caffe框架项目的能力；
  3. 了解CNN可视化技术，如特征图可视化、热力图可视化等，要求会写代码；
  4. 了解语义分割、实例分割、目标跟踪、行人重识别、行为识别、3DCNN模型、视频中的一些检测、Transformer模型等研究方向的经典论文、创新设计思路、优缺点等；
- 学习方式：多看论文、公众号的论文分享；
- 学习目的：扩充知识储备，提高创新能力，加深理解。

## 十、技术路线

- 内容：学习神经网络量化、混合精度训练、模型部署等方面技术，阅读框架的底层实现源码
  - 模型部署相关框架：TVM、TensorRT、CUDA、OpenVINO、MNN、Oneflow、libtorch等；
  - 目标检测方面框架：OpenMMLab、Detectron
  - 源码框架：PyTorch、Caffe
