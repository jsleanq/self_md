# 轻量化模型

## Xception网络（2016）

Inception网络解耦了通道和空间相关性，推导出深度可分离卷积（比较老了），基于Inception网络，距离真正的轻量化模型还有差距，但是相较于Inception在参数量上有减少

## SqueezeNet网络（2016）

轻量化模型优点：

1.较小的 CNNs 在分布式训练过程中需要跨服务器进行更少的通信。

2.较小的 CNNs 需要更少的带宽, 将一个新的模型从云端导出到自动汽车。

3.较小的 CNNs 提供了在 fpga 和其他内存有限的硬件上部署的可行性。

### 设计策略：

策略1. 用1x1卷积替换3x3卷积

策略2. 减少3x3 卷积输入通道的数量

策略3. 在网络中延迟下采样的时间, 以便卷积层具有较大的特征图

### Fire Model

一个Fire模块包括: 

1.一个squeeze层 (只有1x1 卷积),

2.一个具有1x1 和3x3 卷积组合的expand层。

在Fire模块中随意使用1x1 过滤器是应用策略1。在一个Fire模块中有三个超参数: s1x1, e1x1和 e3x3。s1x1 是squeeze层 (所有 1x1) 中的过滤器数, e1x1是1x1 卷积在expand层的数量, e3x3 3x3卷积在expand层的数量,。当我们使用Fire模块时, 我们设置 s1x1 < (e1x1  +  e3x3 ), 因此, expand层有助于限制3x3卷积中输入通道的数量即策略 2。

<img src="https://img-blog.csdnimg.cn/img_convert/edecdebd10f63b72b5f0ed23202ecfdf.png" alt="img" style="zoom: 80%;" />

### SqueezeNet结构

一个conv层后跟着8个fire模块，最后 conv 层结束，逐渐增加每个Fire模块的卷积的数量

![img](https://img-blog.csdnimg.cn/20201025112828708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNDkxMDkyMg==,size_16,color_FFFFFF,t_70)

<img src="https://img-blog.csdnimg.cn/20201025155036316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNDkxMDkyMg==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />



## ShuffleNet网络

### [ShuffleNetV1(2017)](https://blog.csdn.net/weixin_44751294/article/details/116935677)

分组卷积作用：不在一整张特征图上做卷积，而是分组进行卷积，降低计算量（Xception，MobileNet，ResNeXt）。

分组卷积问题：不同组之间的特征图需要通信，否则会降低网络的特征提取能力，所以用1×1卷积调整通道数并进行组间通信，而1×1卷积费时。

#### Channel Shuffle操作

<img src="https://img-blog.csdnimg.cn/20210517154333242.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1MTI5NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 80%;" />

将组内的特征矩阵再分为G组，原来的G份变成了G×G份，对于每个组的相同位置组合在一起重新组成一个channel，之后再进行组卷积就不用1×1卷积调整通道数

#### Shuffle模块

<img src="https://img-blog.csdnimg.cn/20210517155721903.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1MTI5NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 80%;" />

ResNet——把1×1卷积换成1×1的分组卷积并加入channel shuffle模块——有降采样时采用concat进行连接

### [ShuffleNetV2(2018)](https://blog.csdn.net/weixin_44751294/article/details/118187759)

提出观点：通过计算浮点运算量（FLOPs）来描述轻量级网络的快慢。但是从来不考虑运行的速度。

提出标准：在移动设备中的运行速度不仅仅需要考虑FLOPs，还需要考虑内存访问成本(MAC，memory access cost)和平台特点(platform characterics)

论文主要是通过控制不同环境测试模型运行速度

#### 四条准则：

1.当卷积层的输入特征矩阵与输出特征矩阵channel相等时MAC最小（保持FLOPs不变）

2.当GConv的groups增大时，MAC也会增大（保持FLOPs不变）

3.网络设计的碎片化程度越高，速度越慢

4.Element-wise操作带来的影响是不可忽视的（ReLU、shortcut、depthwise convolution）：强调的是不要乱用，用少了影响精度

#### ShuffleV2模块

<img src="https://img-blog.csdnimg.cn/20210624174338622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1MTI5NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 80%;" />

为此，ShuffleNetV2做出了改进，如图( c )所示，在每个单元的开始，c特征通道的输入被分为两个分支（在ShuffleNetV2中这里是对channels均分成两半）。根据G3，不能使用太多的分支，所以其中一个分支不作改变，另外的一个分支由三个卷积组成，它们具有相同的输入和输出通道以满足G1。两个1 × 1卷积不再是组卷积，而改变为普通的1x1卷积操作（**说实话就是又改回去了**），这是为了遵循G2（需要考虑组的代价）。卷积后，两个分支被连接起来，而不是相加(G4)。因此，通道的数量保持不变(G1)。然后使用与ShuffleNetV1中相同的“channels shuffle”操作来启用两个分支之间的信息通信。需要注意，ShuffleNet v1中的“Add”操作不再存在。像ReLU和depthwise convolutions 这样的元素操作只存在于一个分支中。

对于空间下采样，单元稍作修改，移除通道分离操作符。因此，输出通道的数量增加了一倍。具体结构见图（d）。所提出的构建块( c )( d )以及由此产生的网络称为ShuffleNet V2。基于上述分析，我们得出结论，该体系结构设计是高效的，因为它遵循了所有的指导原则。积木重复堆叠，构建整个网络。

#### 结论

ShuffleNetV2网络高效准确的主要原因：

- 首先，每个构建块的高效率使使用更多的特征通道和更大的网络容量成为可能
- 第二，**在每个块中，有一半的特征通道直接穿过该块并加入下一个块。这可以看作是一种特性重用**，就像DenseNet和CondenseNet的思想一样。

## ESPNet网络

### ESPNetV1网络

主要用于语义分割，利用逐点卷积核空洞卷积空间金字塔将标准的卷积进行拆分，之后利用HHF消除空洞卷积的网格效应

![img](https://img-blog.csdnimg.cn/67b210d2e4e446bf85ed60862a483b02.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Zif5Zif5aSq6I-c5LqG,size_15,color_FFFFFF,t_70,g_se,x_16)

#### CNN网络模块对比

![img](https://img-blog.csdnimg.cn/3bb92d1cab2a435e87a8416fa77b1940.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Zif5Zif5aSq6I-c5LqG,size_20,color_FFFFFF,t_70,g_se,x_16)

### ESPNetV2网络（2018）

**主要贡献：**

1.将原来ESPNet的point-wise conv逐点卷积 替换为group point-wise conv组逐点卷积；（分组操作）

2.将原来ESPNet的dilated convolutions 空洞卷积替换为depth-wise dilated convolution深度空洞卷积；（深度可分离）

3.HFF加在depth-wise dilated separable conv深度空洞卷积 和 point-wise (or 1 × 1) 逐点卷积之间,去除Gridding Artifacts ；

4.使用中一个 group point-wise conv组逐点卷积 替换K 个point-wise conv逐点卷积 ；

5.将depth-wise dilated conv 深度空洞卷积加入下采样操作；

6.加入平均池化（average pooling ）,将输入图片信息加入EESP中；

7.使用级联（concatenation） 取代对应元素加法操作（element-wise addition operation ）

#### depth-wise dilated convolution深度空洞卷积

**在MobileNet中引入了深度可分离卷积，它可以有效降低计算量，这里在深度可分离卷积的基础上在卷积上添加空洞系数，从而增大卷积的感受野**

用**分组逐点卷积代替逐点卷积**，用高效的**深度空洞可分离卷积代替3×3空洞卷积代替计算上昂贵的3×3空洞卷积**，使用计算效率高的层次特征融合（HFF）方法对特征图进行融合，消除由扩展卷积引起的网格伪影

![img](https://img-blog.csdnimg.cn/20210129045835257.png)

#### 带有stride的EESP模块

在下采样和卷积（滤波）操作期间，空间信息丢失。为了更好地编码空间关系并有效地学习表示，我们添加了输入图像和当前下采样单元之间的有效long−rangeshortcut连接。这种连接首先对图像进行下采样，使其与特征图的大小相同，然后使用两个卷积的堆栈学习表示。第一个卷积是学习空间表示的标准3×3卷积，而第二个卷积是学习输入之间的线性组合并将其投影到高维空间的逐点卷积

![img](https://img-blog.csdnimg.cn/20210129050948670.png)

## MobileNet网络

### [MobileV1（2017）、V2（2018）](https://blog.csdn.net/qq_37541097/article/details/105771329)

### [Mobilev1、v2、v3（2019）](https://blog.csdn.net/m0_62108334/article/details/125708654?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125708654-blog-105771329.pc_relevant_3mothn_strategy_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125708654-blog-105771329.pc_relevant_3mothn_strategy_recovery&utm_relevant_index=2)

## Mobilenet-SSDv2网络(2019)

以mobilenetv2作为骨干网络，在嵌入式设备上只有SSD和yolo（单级目标检测网络）

SSD考虑多个不同尺度的特征图，适合检测不同类型的对象

### Mobilenet-SSDv2网络架构

Mobilenet-v2网络提供6个不同尺寸的特征图，供后端检测网络进行多尺度目标检测，同时提高检测效率，但是由于提取的特征维数较少，加入[FPN](https://blog.csdn.net/weixin_55073640/article/details/122627966?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122627966-blog-87359191.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122627966-blog-87359191.pc_relevant_aa&utm_relevant_index=2)进行特征融合，

<img src="https://img-blog.csdnimg.cn/2021021314361169.png" alt="img" style="zoom:80%;" />

在骨干网后加入四个反向残差模块，提取1/64、1/128、1/256、1/512比例的特征图



<img src="https://img-blog.csdnimg.cn/20210213144237109.png" alt="img" style="zoom:80%;" />

<img src="https://img-blog.csdnimg.cn/20210213144247905.png" alt="img" style="zoom:80%;" />

## EfficientNet网络

### [EfficientNetv1(2019)](https://blog.csdn.net/qq_37541097/article/details/114434046)

NAS搜索宽度、深度、图像分辨率三者的最优解

#### EfficientNet-B0

<img src="https://img-blog.csdnimg.cn/20210306135337653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" alt="EfficientNetb0" style="zoom:50%;" />

#### MBConv结构

实质：MobileNetV3网络中的`InvertedResidualBlock`（采用了swish激活函数，以及SE模块）

<img src="https://img-blog.csdnimg.cn/20210419135003777.png#pic_center" alt="mbblock" style="zoom:50%;" />

### [EfficientNetv2(2021)](https://blog.csdn.net/qq_37541097/article/details/116933569)

EfficientNetv1问题：

1.训练图像的尺寸很大时，训练速度非常慢（吃显存）

2.在网络浅层中使用Depthwise convolutions速度会很慢

3.同等的放大每个stage是次优的。（对于不同stage的缩放程度不同）

**贡献：**

1.引入新的网络(EfficientNetV2)，该网络在训练速度以及参数数量上都优于先前的一些网络

2.提出了改进的渐进学习方法，该方法会根据训练图像的尺寸动态调节正则方法(例如`dropout`、`data augmentation`和`mixup`)。通过实验展示了该方法不仅能够提升训练速度，同时还能提升准确率

3.通过实验与先前的一些网络相比，训练速度提升11倍，参数数量减少

#### Fused-MBConv

将原来的`MBConv`结构主分支中的`expansion conv1x1`和`depthwise conv3x3`替换成一个普通的`conv3x3`（去掉了之后的”倒残差“过程）：提高处理效率，利用移动端和服务端的加速器

<img src="https://img-blog.csdnimg.cn/20210517161009976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" alt="Fused-MBConv" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/20210519181235595.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" alt="Fused-MBConv" style="zoom:50%;" />

#### EfficientNetV2-S

<img src="https://img-blog.csdnimg.cn/20210517182754449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" alt="EfficientNetv2-s" style="zoom:50%;" />

不同点：

1.浅层stage采用`Fused-MBConv`

2.较小的`expansion ratio`，减少内存访问开销

3.使用更小(`3x3`)的kernel_size，减少感受野，提高网络深度

####  渐进式学习策略

根据不同的输入，调整正则化方法，包括Dropout`，`RandAugment` 以及 `Mixup（训练量过大，实验室条件不足）

## GhostNet网络（2019）

对图像提取出的特征图中存在冗余的情况进行处理，**即插即用模块**

### 目前流行的轻量方法：

#### 模型压缩（性能通常取决于给定的预训练模型）

- pruning connection: 连接剪枝，剪掉一些不重要的神经元连接
- channel pruning: 通道剪枝，剪掉一些无用的通道
- model quantization: 模型量化，在具有离散值的神经网络中对权重或激活函数进行压缩和计算加速
- tensor decomposition: 张量分解，通过利用权重的冗余性和低秩性来减少参数或计算
- knowledge distillation: 知识蒸馏， 利用大模型教小模型，提高小模型的性能

#### 紧凑模型设计（虽然这些模型在很少的FLOPs下获得了良好的性能，但特征映射之间的相关性和冗余性并未得到很好的利用。）

- Xception: depthwise conv operation
- MobileNet v1-v3： depthwise separable conv、 inverted residual block、AutoML technology
- ShuffleNet v1-v2： channel shuffle operation、channel split

### ghost模块

<img src="https://img-blog.csdnimg.cn/48fbeb8646b845f9839a85a5f1d82a97.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA54y05a2Q6K-35p2l55qE5pWR5YW177-9,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom:50%;" />

卷积过程：

第一步: 少量卷积（比如正常用32个卷积核，这里就用16个，从而减少一半的计算量）

第二步：cheap operations，如图中的Φ表示，从问题3中可知，Φ 是诸如3x3 或 5x5的卷积，并且是逐个特征图的进行卷积（Depth-wise convolutional）最后拼在一起

<img src="https://img-blog.csdnimg.cn/20210705143434292.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjUzNzk3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

### Depthwise Separate Conv 和 Ghost Module：

- 深度可分卷积是用深度卷积处理每一个特征通道上的空间信息，然后用点卷积进行通道间的特征融合；
- 而幻象模块是用正常的卷积生成部分真实feature map，再用这些真实的feature map经过线性变换 (Cheap operations ) 得到幻象特征层（Ghost feature map），最终由真实特征层和幻象特征层组成完整特征层

## MicroNet网络（2020）

背景：定义在一个资源十分紧张的场景：在6MFLOPs的限定下进行分辨率为224x224的1000类图片分类

设计要领：1）通过降低特征节点间的连通性来避免网络宽度的减少。2）通过增强非线性能力来补偿网络深度的减少。

### Micro-Factorized Convolution

对MobileNet的深度分离卷积进行更轻量化的改造，对pointwise convolution和depthwise convolution进行低秩近似。

先对输入进行维度压缩，shuffle后进行维度扩展，减少输入输出连接数

![img](https://img-blog.csdnimg.cn/img_convert/3a4482f8ddd439833c29f8a72b791413.png)

### Micro-Factorized Depthwise Convolution

将k × k 深度卷积分解为k × 1 卷积与1 × k 卷积

![img](https://img-blog.csdnimg.cn/img_convert/b9f8c39fce143629b9c35879a54d6bfa.png)

### Combining Micro-Factorized Pointwise and Depthwise Convolutions

两种组合方法：

- 常规组合，直接将两种操作进行组合，这种情况下，两种操作的输入输出维度相同。
- lite组合，如图所示，增加Micro-Factorized Depthwise Convolution的卷积核数，对输入进行维度扩展，然后用Micro-Factorized Pointwise Convolution进行维度压缩。

![img](https://img-blog.csdnimg.cn/img_convert/ac92cbe6219e3f000ab9766bf08525d6.png)

### Dynamic Shift-Max

作为激活函数使用

<img src="https://img-blog.csdnimg.cn/img_convert/e4248b3e11db7a7b839befb4338e671d.png" alt="img" style="zoom: 80%;" />

提升组间的维度交流，弥补MicroFactorized pointwise convolution只专注于组内连接的不足

### MicroNet Architecture

<img src="https://img-blog.csdnimg.cn/img_convert/e6f0e747eb9b1c3f9e4e48d55c568852.png" alt="img" style="zoom: 80%;" />

- Micro-Block-A：使用lite组合，对分辨率较高的低维特征特别有效。
- Micro-Block-B：在Micro-Block-A基础上加了一个 MicroFactorized pointwise convolution进行维度扩展，每个MicroNet仅包含一个Micro-Block-B。
- Micro-Block-C：与Micro-Block-B类似，将lite组合替换为常规组合，能够集中更多的计算在维度融合，如果输入输出维度一样，则增加旁路连接。

网络整体主要有两个核心：Micro-Factorized convolution和Dynamic Shift-Max，Micro-Factorized convolution通过低秩近似将原卷积分解成多个小卷积，保持输入输出的连接性并降低连接数，Dynamic Shift-Max通过动态的组间特征融合增加节点的连接以及提升非线性，弥补网络深度减少带来的性能降低。



[参考链接](https://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&mid=2247499963&idx=2&sn=ebdfe6dc4c28de6fd4c837a113224455&chksm=c1947fa5f6e3f6b3281ae7c38107bd508e5ff681f21a64b439b079c6e93f0e1f4f205b92707f&mpshare=1&scene=23&srcid=10118HAeuUXeYMdG9Qmx1aUf&sharer_sharetime=1665453756314&sharer_shareid=b26d437f4a60a65f3ffbe664568681c3#rd)

## RepVGG 

### MAC(memory access cost) 

内存访问次数，也叫内存使用量，CNN 网络中每个网络层 MAC 的计算分为读输入 feature map 大小、权重大小（DDR 读）和写输出 feature map 大小（DDR 写）三部分。

以卷积层为例计算 MAC，可假设某个卷积层输入 feature map 大小是 (Cin, Hin, Win)，输出 feature map 大小是 (Hout, Wout, Cout)，卷积核是 (Cout, Cin, K, K)，理论 MAC（理论 MAC 一般小于 实际 MAC）计算公式如下：

```python
input = Hin x Win x Cin  # 输入 feature map 大小
output = Hout x Wout x Cout  # 输出 feature map 大小
weights = K x K x Cin x Cout + bias   # bias 是卷积层偏置
ddr_read = input +  weights
ddr_write = output
MAC = ddr_read + ddr_write
```

### ACNet创新点：

ACNet 的创新分为训练和推理阶段：

**训练阶段**：将现有网络中的每一个 3 × 3 卷积层换成 3 × 1 卷积 + 1 × 3 卷积 + 3 × 3 卷积共三个卷积层，并将三个卷积层的计算结果进行相加得到最终卷积层的输出。因为这个过程引入的 1 × 3 卷积和 3 × 1卷积是非对称的，所以将其命名为 Asymmetric Convolution。论文中有实验证明引入 1 × 3 这样的水平卷积核可以提升模型对图像上下翻转的鲁棒性，竖直方向的 3 × 1 卷积核同理。

**推理阶段**：主要是对三个卷积核进行融合，这部分在实现过程中就是使用融合后的卷积核参数来初始化现有的网络。

### RepVGG优势

- 3x3 卷积非常快。在GPU上，3x3 卷积的计算密度（理论运算量除以所用时间）可达 1x1 和 5x5 卷积的四倍。
- 单路架构非常快，因为并行度高。同样的计算量，“大而整”的运算效率远超“小而碎”的运算。已有研究表明：并行度高的模型要比并行度低的模型推理速度更快。
- 单路架构省内存。例如，ResNet 的 shortcut 虽然不占计算量，却增加了一倍的显存占用。
- 单路架构灵活性更好，容易改变各层的宽度（如剪枝）。
- RepVGG 主体部分只有一种算子：3x3 卷积接 ReLU。在设计专用芯片时，给定芯片尺寸或造价，可以集成海量的 3x3 卷积-ReLU 计算单元来达到很高的效率，同时单路架构省内存的特性也可以帮我们少做存储单元。

<img src="https://img-blog.csdnimg.cn/a01f97ee17d047349f60b459555d1ec2.png" alt="图2" style="zoom:50%;" />

多分支结构以对于推理不友好，但对于训练友好，本文作者提出一种新思想：**训练一个多分支模型，推理时将多分支模型等价转换为单路模型**

在训练时候，采用：*y*=*x*+*g*(*x*)+*f*(*x*)，*x*、*g*(*x*)、*f*(*x*)分别对应恒等映射，1 × 1 卷积，3 × 3 卷积

在推理阶段，上述模块转换成：*y*=*h*(*x*)， *h*(*x*)的参数可以通过线性组合方式从训练好的模型中转换得到

#### 结构重参数化

1.conv+BN

2.1\*1 转换成 3*3 

## VovNet

CNN 模型的落地准则：精度和速度

### 结构优势

![img](https://pic3.zhimg.com/v2-11bdc0bf20bc145879ae65db211ab7a6_b.jpg)

<img src="https://pic1.zhimg.com/v2-d1a8633ade615931dbf76d0a4874a90c_b.jpg" alt="img" style="zoom:50%;" />

第一个stage：包含3个3×3卷积，channel 数分别是64, 64和128

第二个stage：经过了1个 OSA 模块，输出特征大小变为原来的1/4了，包含5个3×3卷积，和一个1×1卷积（用于修改通道数）

#### OSA模块：

OSA（One-Shot Aggregation）：**指 OSA 模块的 concat 操作只进行一次，即只有最后一层的输入是前面所有层 feature map 的 concat（叠加）**。OSA 模块的结构图如图 (b) 所示：

![img](https://pic2.zhimg.com/v2-245571b8e1401612a2d9d2026f81507d_b.jpg)

#### DenseNet 缺点（导致了能耗和推理效率低的）：

- 密集连接会增加输入通道大小，但输出通道大小保持不变，导致的输入和输出通道数都不相等。因此，DenseNet 具有具有较高的 MAC。
- DenseNet 采用了 bottleneck 结构，这种结构将一个 3 × 3 卷积分成了两个计算（1x1+3x3 卷积），这带来了更多的序列计算（sequential computations），导致会降低推理速度。



